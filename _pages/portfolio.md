---
layout: archive
title: "Portfolio"
permalink: /portfolio/
author_profile: true
redirect_from:

- /resume

---

{% include base_path %}



<hr style="height:4px; background-color: #696969; margin-bottom:-2.5em; ">


<div style="clear: both;">
    <div style="float: right;vertical-align: middle;">
        <a href="https://psyarxiv.com/7enqw/">
            <img src="../files/PsyArXiv_logo.png" style="height:2em;float:right" alt="">
        </a>
    </div>
    <div>
        <h2 style="font-size:1.5em;">Predicting pandemic mental health outcomes</h2>
    </div>
</div>

<img src="../files/covid19_fig.png" width="30%" style="float:left;padding-right:0.5em">
<p style="font-size:1em;">The “Mental Health Impact of COVID-19 Pandemic on NIMH Patients and Volunteers” study was a longitudinal study launched in spring 2020 by researchers at NIMH, to investigate the effect of the emerging COVID-19 pandemic on mental health. For each participant, the study collected personal characteristics, such as demographics, psychological traits, and clinical history, together with personal circumstances at regular intervals during their enrollment in the study. In this paper, we examine the degree to which a variety of mental health outcomes over time for an individual can be predicted from personal characteristics and their changing circumstances, using regression models trained on other study participants. We find that it is possible to predict the variation of a participant's mental health outcomes from time point to time point, for most of the outcomes we consider. This capability is dominated by information about outcome at the point of enrollment in the study, but can be improved by considering personal characteristics and circumstances.</p>

<hr style="height:4px; background-color: #696969; margin-bottom:-2.5em; margin-top:-1em">

<div style="clear: both;">
    <div style="float: right;vertical-align: middle;">
        <a href="https://github.com/carlwharris/nwb-photostim">
            <img src="../files/GitHub_logo.png" style="height:2em">
        </a>
  </div>
  <div>
    <h2 style="font-size:1.5em;">Holographic photostimulation extension</h2>
</div>
</div>

<img src="../files/nwb_overview.png" width="50%" style="float:left;padding-right:0.5em">
<p style="font-size:1em;">State-of-the-art <a href="https://www.nature.com/articles/s41467-017-01031-3">holographic photostimulation methods</a>, 
used in concert with <a href="https://www.nature.com/articles/nmeth818">two-photon imaging</a>, allow unprecedented 
control and measurement of cell activity in the living brain. Methods for managing data for two-photon imaging 
experiments are improving, but there is little to no standardization of data for holographic stimulation methods. 
Stimulation in vivo depends on fine-tuning many experimental variables, which poses a challenge for reproducibility 
and data sharing between researchers. To improve <a href="https://www.sciencedirect.com/science/article/pii/S0896627321009557">standardization</a> of photostimulation data storage and processing, 
we release this extension as a generic data format for simultaneous holographic stimulation experiments, 
using the <a href="https://www.nwb.org/">NeuroData Without Borders (NWB)</a> format for neurophysiology data to store experimental details and data relating to both acquisition 
and photostimulation. It includes <a href="https://pynwb.readthedocs.io/en/stable/">containers</a> for storing photostimulation-specific device parameters, holographic patterns (either 2D or 3D), and time series data related to photostimulation. This project is part of an ongoing intra-NIMH collaboration between <a href="https://markhisted.org/">Mark Histed's lab</a> and the <a href="https://cmn.nimh.nih.gov/dsst">Data Science and Sharing Team</a>.</p>

<hr style="height:4px; background-color: #696969; margin-bottom:-2.5em; margin-top:-1em">

<div style="clear: both;">
    <div style="float: right;vertical-align: middle;">
        <a href="https://arxiv.org/abs/2211.09295">
            <img  src="../files/arxiv-logo-1.png" style="height:2em">
        </a>
    </div>
    <div>
        <h2 style="font-size:1.5em;">Context-dependent encoding</h2>
    </div>
</div>

<img src="../files/boxplots_v2.png" style="float:right;padding-right:0.5em" width="40%">
<p style="font-size:1em;">We propose a decoding-based approach to detect context effects on neural codes in longitudinal neural recording data. The approach is agnostic to how information is encoded in neural activity, and can control for a variety of possible confounding factors present in the data. We demonstrate our approach by determining whether it is possible to decode location encoding from prefrontal cortex in the mouse and, further, testing whether the encoding changes due to task engagement.</p>

<hr style="height:4px; background-color: #696969; margin-bottom:-2.5em; margin-top:-1em">

<div style="clear: both;">
    <div style="float: right;vertical-align: middle;">
        <a href="https://www.biorxiv.org/content/10.1101/2022.06.20.496909v1">
            <img src="https://www.biorxiv.org/sites/default/files/site_logo/bioRxiv_logo_homepage.png" style="height:2em;">
        </a>
        &nbsp;&nbsp;
        <a href="https://github.com/carlwharris/DeepAction" >
            <img align="right" src="../files/GitHub_logo.png" style="height:2em;">
        </a>
    </div>
    <div>
        <h2 style="font-size:1.5em;">DeepAction Toolbox</h2>
    </div>
</div>

<div style="clear: both;padding-top: 0.25em">
  <div style="float: right; width:50%;padding-left:0.5em">
   <table style="border:none;">
    <tr style="border:none">
        <td style="border:none;padding:0.2em" width="33%">
			<img src="../files/home_cage_50.gif" style="max-width:100%;height:auto;">
		</td>
		<td style="border:none;padding:0.2em" width="33%">
			<img src="../files/CRIM13S-785.gif" style="max-width:100%;height:auto;">
		</td>						
		<td style="border:none;padding:0.2em" width="33%">
			<img src="../files/CRIM13T-203.gif" style="max-width:100%;height:auto;">
		</td>
	</tr>
	<tr style="border:none;">
		<td style="border:none;padding:0.2em;" width="33%">
			<img src="../files/home_cage_182.gif" style="max-width:100%;height:auto;">
		</td>
		<td style="border:none;padding:0.2em" width="33%">
			<img src="../files/CRIM13S-1785.gif" style="max-width:100%;height:auto;">
		</td>						
		<td style="border:none;padding:0.2em" width="33%">
			<img src="../files/CRIM13T-256.gif" style="max-width:100%;height:auto;">
		</td>
	</tr>
    </table>
  </div>
  <div >
    <p style="font-size:1em;margin-top: -0.5em">The identification of animal behavior in video is a critical but time-consuming task in 
    many areas of research. Here, we introduce DeepAction, a deep learning-based toolbox for automatically annotating 
    animal behavior in video. Our approach uses features extracted from raw video frames by a pretrained convolutional 
    neural network to train a recurrent neural network classifier. We evaluate the classifier on two benchmark rodent 
    datasets and one octopus dataset. We show that it achieves high accuracy, requires little training data, and 
    surpasses both human agreement and most comparable existing methods. We also create a confidence score for 
    classifier output, and show that our method provides an accurate estimate of classifier performance and reduces the 
    time required by human annotators to review and correct automatically-produced annotations. We release our system 
    and accompanying annotation interface as an open-source MATLAB toolbox.</p>
  </div>
</div>

<hr style="height:4px; background-color: #696969; margin-bottom:-2.5em; margin-top:-1em">

<div style="clear: both;">
  <div style="float: right;vertical-align: middle;">
   <a href="https://github.com/carlwharris/Discovery-DLC-processing">
	<img  src="../files/GitHub_logo.png" style="height:2em">
</a>
  </div>
  <div>
    <h2 style="font-size:1.5em;">HPC keypoint extraction</h2>
  </div>
</div>

<img src="../files/pipeline_diagram.png" style="width:60%;float:right;padding-left:0.5em">
<p style="font-size:1em;">The purpose of this project is to enable the extraction of animal keypoints from very large video datasets via DeepLabCut, on the Dartmouth College <a href="https://rc.dartmouth.edu/index.php/discovery-overview/">Discovery HPC cluster</a>. Videos are recorded & uploaded to a folder in the user's DropBox account, after which the user selects a subset to extract annotations for. For subset, corresponding video files are first downloaded from DropBox onto the user's HPC accounts using an API key. Then, a trained DeepLabCut network is used to extract keypoints from the video. After keypoints have been extracted, the video is deleted, to clear space for the rest of the video files in the batch to be downloaded. In the final step, keypoints uploaded from the HPC to the user's specified DropBox folder.</p>

<hr style="height:4px; background-color: #696969; margin-bottom:-2.5em; margin-top:-1em">


<div style="clear: both;">
    <div style="float: right;vertical-align: middle;">
        <a href="https://github.com/carlwharris/elliptic-curve-cryptosystems">
            <img align="right" src="../files/GitHub_logo.png" style="height:2em">
        </a>
    </div>
    <div>
        <h2 style="font-size:1.5em;">Elliptic curve cryptography</h2>
    </div>
</div>

<img src="../files/ECC.jpeg"  style="width:15%;float:right;padding-left:0.5em">
<p style="font-size:1em;">Final project for Abstract Algebra (Dartmouth College, Fall 2020) in which I implemented a simple elliptic curve cryptosystem in MATLAB. The corresponding <a href="https://github.com/carlwharris/elliptic-curve-cryptosystems/blob/main/ECC%20Project%20Paper.pdf">paper</a> includes information about: the invention of public key and elliptic curve cryptography, elliptic curves over finite fields, subgroup generation, and how cryptographic systems are constructed from elliptic curves and used to encrypt and decrypt messages.</p>


<hr style="height:4px; background-color: #696969; margin-top:-1em">
